{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6369704,"sourceType":"datasetVersion","datasetId":3669898},{"sourceId":7457404,"sourceType":"datasetVersion","datasetId":3678957}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project: Human Activity Recognition using accelometer and gyroscope data**\n\n# **Â© Copyright: Utsab Saha and Sawradip Saha**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random  # Import the random module\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(seed=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activities are the class labels\n# It is a 6 class classification\nACTIVITIES = {\n    0: 'WALKING',\n    1: 'WALKING_UPSTAIRS',\n    2: 'WALKING_DOWNSTAIRS',\n    3: 'SITTING',\n    4: 'STANDING',\n    5: 'LAYING',\n}\n\n# Utility function to print the confusion matrix\ndef confusion_matrix(Y_true, Y_pred):\n    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n\n    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data directory\nDATADIR = '/kaggle/input/har-data-uci/UCI HAR Dataset'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIGNALS = [\n    \"body_acc_x\",\n    \"body_acc_y\",\n    \"body_acc_z\",\n    \"body_gyro_x\",\n    \"body_gyro_y\",\n    \"body_gyro_z\",\n    \"total_acc_x\",\n    \"total_acc_y\",\n    \"total_acc_z\"\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to read the data from csv file\ndef _read_csv(filename):\n    return pd.read_csv(filename, delim_whitespace=True, header=None)\n\ndef load_signals(subset):\n    signals_data = []\n\n    for signal in SIGNALS:\n        filename = f'/kaggle/input/har-data-uci/UCI HAR Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n        signals_data.append(\n            _read_csv(filename).values\n        ) \n\n    # Transpose is used to change the dimensionality of the output,\n    # aggregating the signals by combination of sample/timestep.\n    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n    return np.transpose(signals_data, (1, 2, 0))\n\ndef load_y(subset):\n    filename = f'/kaggle/input/har-data-uci/UCI HAR Dataset/{subset}/y_{subset}.txt'\n    y = _read_csv(filename)[0]\n\n    return y\ndef load_data():\n    X_train, X_test = load_signals('train'), load_signals('test')\n    y_train, y_test = load_y('train'), load_y('test')\n\n    return X_train, X_test, y_train, y_test\n\n# function to count the number of classes\ndef _count_classes(y):\n    return len(set([tuple(category) for category in y]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the train and test data\nX_train, X_test, y_train, y_test = load_data()\ny_train  = y_train - 1\ny_test  = y_test - 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting X_train_static and y_train_static into training and test sets\ntest_size = 0.2\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_val shape:\", X_val.shape)\nprint(\"y_val shape:\", y_val.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyHARDataset(Dataset):\n    def __init__(self, feat_matrix, y_feat):\n        self.feat_matrix = feat_matrix\n        self.y_feat = y_feat\n        \n    def __len__(self):\n        return self.feat_matrix.shape[0]\n\n    def __getitem__(self, idx):\n        feat_x = self.feat_matrix[idx].transpose()\n        feat_y = self.y_feat.values[idx]\n        return torch.from_numpy(feat_x).float(), feat_y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_data = MyHARDataset(X_train, y_train)\nval_data = MyHARDataset(X_val , y_val)\ntrain_dataloader = DataLoader(training_data, batch_size=10, shuffle=True)\nval_dataloader = DataLoader(val_data, batch_size=10, shuffle=True)\n\ntest_data = MyHARDataset(X_test,y_test)\ntest_dataloader = DataLoader(test_data,batch_size=10, shuffle=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for xd, yd in train_dataloader :\n    print(yd)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weight_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n        nn.init.zeros_(m.bias)\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conv_layers, batch_norm=True):\n        super(ResNetBlock, self).__init__()\n\n        # The first convolutional layer\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n        if batch_norm:\n            self.bn1 = nn.BatchNorm1d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Additional convolutional layers\n        layers = []\n        for _ in range(num_conv_layers - 1):\n            layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1))\n            if batch_norm:\n                layers.append(nn.BatchNorm1d(out_channels))\n            layers.append(nn.ReLU(inplace=True))\n        \n        # The final convolutional layer\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n        if batch_norm:\n            self.bn2 = nn.BatchNorm1d(out_channels)\n\n        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n        \n        # Adjust the dimensions of the skip connection\n        self.identity_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        identity = self.identity_conv(x)  # Adjusted skip connection\n        out = self.conv1(x)\n        if hasattr(self, 'bn1'):\n            out = self.bn1(out)\n        out = self.relu(out)\n        \n        out = self.conv2(out)\n        if hasattr(self, 'bn2'):\n            out = self.bn2(out)\n        \n        # Apply skip connection\n        out += identity\n        out = self.relu(out)\n        \n        out = self.maxpool(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet, self).__init__()\n\n        self.block1 = ResNetBlock(9, 64, num_conv_layers=2)\n        self.block2 = ResNetBlock(64, 128, num_conv_layers=2)\n        self.block3 = ResNetBlock(128, 256, num_conv_layers=3)\n        self.block4 = ResNetBlock(256, 512, num_conv_layers=3)\n        self.block5 = ResNetBlock(512, 512, num_conv_layers=3)\n\n        self.avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n        self.fc1 = nn.Linear(512, 3)  # Adjusted input size based on the layers\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x= self.softmax(x)\n#         print(x.shape)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FusionActNet(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super(FusionActNet, self).__init__()\n        self.conv1 = nn.Conv1d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n        self.bn1 = nn.BatchNorm1d(in_channels)\n        self.relu = nn.ReLU(inplace=True)\n        \n        self.conv2 = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm1d(out_channels)\n        \n        self.identityconv = nn.Conv1d(in_channels,out_channels,kernel_size=1)\n    def forward(self, x):\n        identity = x\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.conv2(x)\n        x = self.bn2(x)\n\n#         if identity.size(1) != x.size(1):\n#             identity = nn.Conv1d(identity.size(1), x.size(1), kernel_size=1).to(x.device)(identity)\n        identity = self.identityconv(identity)    \n        x += identity\n        \n        return x\n\nclass MyDeeperFusionActNet(nn.Module):\n    def __init__(self):\n        super(MyDeeperFusionActNet, self).__init__()\n\n        # Load pretrained static and dynamic models\n        self.model_static = ResNet()\n        self.model_static.load_state_dict(torch.load('/kaggle/input/har-uci-weights/new_static.pth'))\n        self.model_static.train(False)\n        \n        self.model_dynamic = ResNet()\n        self.model_dynamic.load_state_dict(torch.load('/kaggle/input/har-uci-weights/new_dynamic.pth'))\n        self.model_dynamic.train(False)\n        \n        # MobileNetV1-style layers\n        self.conv1 = nn.Conv1d(9, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm1d(32)\n        self.relu = nn.ReLU(inplace=True)\n        self.block1 = self.make_block(32, 64, 2)  # Depthwise: 2, Pointwise: 2\n        self.block2 = self.make_block(64, 128, 3)  # Depthwise: 3, Pointwise: 3\n        self.block3 = self.make_block(128, 256, 3)  # Depthwise: 3, Pointwise: 3\n        \n        self.avgpool = nn.AdaptiveAvgPool1d(1)\n        self.fc = nn.Linear(256, 1)\n        self.sigmoid = nn.Sigmoid()\n#         self.linear = nn.Linear(6,6)\n#         self.softmax = nn.Softmax(dim=1)\n\n    def make_block(self, in_channels, out_channels, num_blocks):\n        layers = []\n        for _ in range(num_blocks):\n            layers.append(FusionActNet(in_channels, out_channels, stride=1))\n            in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x_static = self.model_static(x)\n        x_dynamic = self.model_dynamic(x)\n#         print(x_static.shape)\n#         print(x_dynamic.shape)\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        \n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        \n        x = self.fc(x)\n        x_weight = self.sigmoid(x)\n#         print(x_weight.shape)\n        x_almost_out = torch.cat((x_static * x_weight, x_dynamic * (1 - x_weight)), -1)\n        return x_almost_out","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = MyDeeperFusionActNet().cuda()\nmodel.apply(weight_init)\nmodel(torch.zeros(10, 9, 128).cuda())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define loss function\n\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.000001)\n\n# Training loop\ndef train(model, train_loader, optimizer, criterion):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total_samples = 0\n    for batch_idx, (x, y) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model(x.cuda())\n        loss = criterion(outputs, y.cuda())\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == y.cuda()).sum().item()\n            total_samples += y.size(0)\n    train_loss = total_loss / len(train_loader)\n    train_accuracy = correct / total_samples\n    return train_loss ,train_accuracy\n\n# Validation loop\ndef validate(model, val_loader, criterion):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(val_loader):\n            outputs = model(x.cuda())\n            loss = criterion(outputs, y.cuda())\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == y.cuda()).sum().item()\n            total_samples += y.size(0)\n    \n    val_loss = total_loss / len(val_loader)\n    val_accuracy = correct / total_samples\n    return val_loss, val_accuracy\n\n# Training and validation loop\nnum_epochs = 60\nfor epoch in range(num_epochs):\n    train_loss, train_accuracy = train(model, train_dataloader, optimizer, criterion)\n    val_loss, val_accuracy = validate(model, test_dataloader, criterion)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n          f\"Train Loss: {train_loss:.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.4f}, \"\n          f\"Val Loss: {val_loss:.4f}, \"\n          f\"Val Accuracy: {val_accuracy:.4f}\")\n    \n    # Save the model weights if validation accuracy improves\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        best_model_weights = model.state_dict()\n        torch.save(best_model_weights, 'best_combined_model_resnet_weights.pth')\n\n# Load the best model weights\nmodel.load_state_dict(best_model_weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model weights\n# model.load_state_dict(torch.load('/kaggle/working/best_combined_model_resnet_weights.pth'))\nmodel.eval()\n\n# Testing loop\ntest_correct = 0\ntotal_test_samples = 0\n\nwith torch.no_grad():\n    for batch_idx, (x_test, y_test) in enumerate(test_dataloader):\n        outputs = model(x_test.cuda())\n        _, predicted = torch.max(outputs, 1)\n        test_correct += (predicted == y_test.cuda()).sum().item()\n        total_test_samples += y_test.size(0)\n\ntest_accuracy = test_correct / total_test_samples\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\n# Load the best model weights\nmodel.load_state_dict(torch.load('/kaggle/input/har-uci-weights/new_combined.pth'))\nmodel.eval()\n\n# Initialize lists to store true labels and predicted labels\ntrue_labels = []\npredicted_labels = []\n\nwith torch.no_grad():\n    for batch_idx, (x_test, y_test) in enumerate(test_dataloader):\n        outputs = model(x_test.cuda())\n        _, predicted = torch.max(outputs, 1)\n        \n        true_labels.extend(y_test.cpu().numpy())\n        predicted_labels.extend(predicted.cpu().numpy())\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\n# Calculate precision, recall, and F1 score\nreport = classification_report(true_labels, predicted_labels, target_names=[\"class_0\", \"class_1\", \"class_2\",\"class_3\",\"class_4\",\"class_5\"])\n\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\nprint(\"\\nClassification Report:\")\nprint(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}