{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6369704,"sourceType":"datasetVersion","datasetId":3669898},{"sourceId":7457404,"sourceType":"datasetVersion","datasetId":3678957},{"sourceId":161804392,"sourceType":"kernelVersion"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Project: Human Activity Recognition using accelometer and gyroscope data**\n\n# **Â© Copyright: Utsab Saha and Sawradip Saha**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random  # Import the random module\nimport torch\nimport os\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(seed=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Activities are the class labels\n# It is a 6 class classification\nACTIVITIES = {\n    0: 'WALKING',\n    1: 'WALKING_UPSTAIRS',\n    2: 'WALKING_DOWNSTAIRS',\n    3: 'SITTING',\n    4: 'STANDING',\n    5: 'LAYING',\n}\n\n# Utility function to print the confusion matrix\ndef confusion_matrix(Y_true, Y_pred):\n    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n\n    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data directory\nDATADIR = '/kaggle/input/har-data-uci/UCI HAR Dataset'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIGNALS = [\n    \"body_acc_x\",\n    \"body_acc_y\",\n    \"body_acc_z\",\n    \"body_gyro_x\",\n    \"body_gyro_y\",\n    \"body_gyro_z\",\n    \"total_acc_x\",\n    \"total_acc_y\",\n    \"total_acc_z\"\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to read the data from csv file\ndef _read_csv(filename):\n    return pd.read_csv(filename, delim_whitespace=True, header=None)\n\ndef load_signals(subset):\n    signals_data = []\n\n    for signal in SIGNALS:\n        filename = f'/kaggle/input/har-data-uci/UCI HAR Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n        signals_data.append(\n            _read_csv(filename).values\n        ) \n\n    # Transpose is used to change the dimensionality of the output,\n    # aggregating the signals by combination of sample/timestep.\n    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n    return np.transpose(signals_data, (1, 2, 0))\n\ndef load_y(subset):\n    filename = f'/kaggle/input/har-data-uci/UCI HAR Dataset/{subset}/y_{subset}.txt'\n    y = _read_csv(filename)[0]\n\n    return y\ndef load_data():\n    X_train, X_test = load_signals('train'), load_signals('test')\n    y_train, y_test = load_y('train'), load_y('test')\n\n    return X_train, X_test, y_train, y_test\n\n# function to count the number of classes\ndef _count_classes(y):\n    return len(set([tuple(category) for category in y]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the train and test data\nX_train, X_test, y_train, y_test = load_data()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape,y_train.shape)\nprint(X_test.shape,y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Static Dataset Preparation for Static Model **","metadata":{}},{"cell_type":"code","source":"import random\n\nstatic_1 = np.where(y_train == 4)[0]\nstatic_2 = np.where(y_train == 5)[0]\nstatic_3 = np.where(y_train == 6)[0]\nstatic = np.concatenate([static_1, static_2, static_3])\nstatic_list = static.tolist()\n\n# Shuffle dynamic data index\nr = random.random()\nrandom.shuffle(static_list, lambda: r)\n\nstatic = np.array(static_list)\n\nX_train_static = X_train[static]\ny_train_static = y_train[static]\n\n# Convert (1, 2, 3) labels to (0, 1, 2)\ny_train_static  = y_train_static - 4\n\nprint (\"\\n+++ DATA STATISTICS +++\\n\")\nprint (\"train_static shape: \", X_train_static.shape,y_train_static.shape)\n\n# Select dynamic HAR test data\n\nstatic_1 = np.where(y_test == 4)[0]\nstatic_2 = np.where(y_test == 5)[0]\nstatic_3 = np.where(y_test == 6)[0]\nstatic = np.concatenate([static_1, static_2, static_3])\n\nX_test_static = X_test[static]\ny_test_static = y_test[static]\n\n# Convert (1, 2, 3) labels to (0, 1, 2)\ny_test_static  = y_test_static - 4\n\nprint (\"test_static shape: \", X_test_static.shape,y_test_static.shape)\n\nn_classes = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Assuming X_train_static and y_train_static are your data arrays\nX_train_static, y_train_static = X_train_static, y_train_static\n\n# Splitting X_train_static and y_train_static into training and test sets\ntest_size = 0.2\nX_train_static, X_val_static, y_train_static, y_val_static = train_test_split(X_train_static, y_train_static, test_size=test_size, random_state=42)\n\nprint(\"X_train_static shape:\", X_train_static.shape)\nprint(\"y_train_static shape:\", y_train_static.shape)\nprint(\"X_val_static shape:\", X_val_static.shape)\nprint(\"y_val_static shape:\", y_val_static.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyHARDataset(Dataset):\n    def __init__(self, feat_matrix, y_feat):\n        self.feat_matrix = feat_matrix\n        self.y_feat = y_feat\n        \n    def __len__(self):\n        return self.feat_matrix.shape[0]\n\n    def __getitem__(self, idx):\n        feat_x = self.feat_matrix[idx].transpose()\n        feat_y = self.y_feat.values[idx]\n        return torch.from_numpy(feat_x).float(), feat_y\n    \ntraining_data_static = MyHARDataset(X_train_static, y_train_static)\nval_data_static = MyHARDataset(X_val_static, y_val_static)\ntrain_dataloader_static = DataLoader(training_data_static, batch_size=10, shuffle=True)\nval_dataloader_static = DataLoader(val_data_static, batch_size=10, shuffle=False)\n\ntest_data_static = MyHARDataset(X_test_static,y_test_static)\ntest_dataloader_static = DataLoader(test_data_static,batch_size=10, shuffle=False)\n\nfor td in train_dataloader_static:\n    x, y = td\n    print(x.shape)\n    print(y)   \n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef weight_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('relu'))\n        nn.init.zeros_(m.bias)\n\nclass ResNetBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_conv_layers, batch_norm=True):\n        super(ResNetBlock, self).__init__()\n\n        # The first convolutional layer\n        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n        if batch_norm:\n            self.bn1 = nn.BatchNorm1d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Additional convolutional layers\n        layers = []\n        for _ in range(num_conv_layers - 1):\n            layers.append(nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1))\n            if batch_norm:\n                layers.append(nn.BatchNorm1d(out_channels))\n            layers.append(nn.ReLU(inplace=True))\n        \n        # The final convolutional layer\n        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n        if batch_norm:\n            self.bn2 = nn.BatchNorm1d(out_channels)\n\n        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=2)\n        \n        # Adjust the dimensions of the skip connection\n        self.identity_conv = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n        \n    def forward(self, x):\n        identity = self.identity_conv(x)  # Adjusted skip connection\n        out = self.conv1(x)\n        if hasattr(self, 'bn1'):\n            out = self.bn1(out)\n        out = self.relu(out)\n        \n        out = self.conv2(out)\n        if hasattr(self, 'bn2'):\n            out = self.bn2(out)\n        \n        # Apply skip connection\n        out += identity\n        out = self.relu(out)\n        \n        out = self.maxpool(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self):\n        super(ResNet, self).__init__()\n\n        self.block1 = ResNetBlock(9, 64, num_conv_layers=2)\n        self.block2 = ResNetBlock(64, 128, num_conv_layers=2)\n        self.block3 = ResNetBlock(128, 256, num_conv_layers=3)\n        self.block4 = ResNetBlock(256, 512, num_conv_layers=3)\n        self.block5 = ResNetBlock(512, 512, num_conv_layers=3)\n\n        self.avgpool = nn.AdaptiveAvgPool1d(1)  # Global average pooling\n        self.fc1 = nn.Linear(512, 3)  # Adjusted input size based on the layers\n        self.softmax = nn.Softmax(dim=1)\n    def forward(self, x):\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc1(x)\n        x= self.softmax(x)\n#         print(x.shape)\n        return x\n\n# Instantiate the ResNet model\nmodel = ResNet().cuda()\nmodel.apply(weight_init)\noutput = model(torch.zeros(10, 9, 128).cuda())\nprint(output.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***Run this Cell only when you want to train using the static data***","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Training loop\ndef train(model, train_loader, optimizer, criterion):\n    model.train()\n    total_loss = 0.0\n    correct = 0\n    total_samples = 0\n    for batch_idx, (x, y) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model(x.cuda())\n        loss = criterion(outputs, y.cuda())\n        loss.backward()\n        optimizer.step()\n        with torch.no_grad():\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == y.cuda()).sum().item()\n            total_samples += y.size(0)\n    train_loss = total_loss / len(train_loader)\n    train_accuracy = correct / total_samples\n    return train_loss ,train_accuracy\n\n# Validation loop\ndef validate(model, val_loader, criterion):\n    model.eval()\n    total_loss = 0.0\n    correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for batch_idx, (x, y) in enumerate(val_loader):\n            outputs = model(x.cuda())\n            loss = criterion(outputs, y.cuda())\n            total_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == y.cuda()).sum().item()\n            total_samples += y.size(0)\n    \n    val_loss = total_loss / len(val_loader)\n    val_accuracy = correct / total_samples\n    return val_loss, val_accuracy\n\nimport torch.optim as optim\n\nbest_val_accuracy = 0.0\nbest_model_weights = None\n\n# Training and validation loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    train_loss, train_accuracy = train(model, train_dataloader_static, optimizer, criterion)\n    val_loss, val_accuracy = validate(model, test_dataloader_static, criterion)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n          f\"Train Loss: {train_loss:.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.4f}, \"\n          f\"Val Loss: {val_loss:.4f}, \"\n          f\"Val Accuracy: {val_accuracy:.4f}\")\n    \n    # Save the model weights if validation accuracy improves\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        best_model_weights = model.state_dict()\n        torch.save(best_model_weights, 'best_static_model_weights.pth')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the best model weights\n# model.load_state_dict(torch.load('/kaggle/working/best_static_model_weights.pth'))\nmodel.eval()\n\n# Testing loop\ntest_correct = 0\ntotal_test_samples = 0\n\nwith torch.no_grad():\n    for batch_idx, (x_test, y_test) in enumerate(test_dataloader_static):\n        outputs = model(x_test.cuda())\n        _, predicted = torch.max(outputs, 1)\n        test_correct += (predicted == y_test.cuda()).sum().item()\n        total_test_samples += y_test.size(0)\n\ntest_accuracy = test_correct / total_test_samples\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}